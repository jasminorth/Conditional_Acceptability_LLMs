{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645a8071",
   "metadata": {},
   "source": [
    "# Conditional Acceptability in Large Language Models and Humans - Post-Hoc Analyses\n",
    "\n",
    "This notebook is used to perform additional analyses, i.e.,\n",
    "* analysis of rating consistency by computing the intraclass correlation\n",
    "* correlation analysis of our ratings to the models' sentence probability and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6ad35",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b764e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e5b50",
   "metadata": {},
   "source": [
    "## Rating Consistency\n",
    "\n",
    "As LLM inference is non-deterministic, the consistency of numerical ratings is an important aspect to consider. In fact, to account for potential instabilities in model ratings, we conduct repeated trials by prompting each model five times per data sample.\n",
    "\n",
    "Additionally, we perform an analysis of rating consistency by computing the intraclass correlation across the five model scores per sample for each score type and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    c_prob  if_prob  if_acc\n",
      "llama3 (vanilla)    0.7685   0.7724  0.7529\n",
      "llama70b (vanilla)  0.9468   0.9344  0.9201\n",
      "qwen2 (vanilla)     0.9768   0.9716  0.9450\n",
      "qwen72b (vanilla)   0.9693   0.9673  0.9784\n",
      "llama3 (fewshot)    0.7930   0.7538  0.7596\n",
      "llama70b (fewshot)  0.9438   0.9556  0.8820\n",
      "qwen2 (fewshot)     0.9542   0.9556  0.9596\n",
      "qwen72b (fewshot)   0.9559   0.9437  0.9768\n",
      "llama70b (cot)      0.9014   0.8497  0.8757\n",
      "qwen72b (cot)       0.9331   0.9127  0.9661\n"
     ]
    }
   ],
   "source": [
    "models = [\"llama3\", \"llama70b\", \"qwen2\", \"qwen72b\"]\n",
    "metrics = [\"vanilla\", \"fewshot\", \"cot\"]\n",
    "\n",
    "collections = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "    \n",
    "        if metric == \"cot\" and model in [\"llama3\", \"qwen2\"]:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(f\"dataframe_{model}_context_{metric}.csv\", sep=\";\")\n",
    "\n",
    "        icc_overall = {}\n",
    "\n",
    "        for var in [\"c_prob\", \"if_prob\", \"if_acc\"]:\n",
    "            # Check if target has more than 1 rater (expected but still a nice safeguard )\n",
    "            counts = df.groupby(\"sample_number\")[\"instance_id\"].count()\n",
    "            valid_samples = counts[counts > 1].index\n",
    "            df_valid = df[df[\"sample_number\"].isin(valid_samples)]\n",
    "\n",
    "            # Reshape datframe for pingouin\n",
    "            long = df_valid[[var, \"instance_id\", \"sample_number\"]].rename(\n",
    "                columns={\"instance_id\":\"rater\", \"sample_number\":\"target\", var:\"score\"}\n",
    "            )\n",
    "\n",
    "            icc = pg.intraclass_corr(data=long, targets=\"target\", raters=\"rater\", ratings=\"score\")\n",
    "            \n",
    "            icc_overall[var] = icc.loc[icc[\"Type\"]==\"ICC2\", \"ICC\"].values[0]\n",
    "\n",
    "        collections[f\"{model} ({metric})\"] = icc_overall\n",
    "print(pd.DataFrame(collections).transpose().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6dba9a",
   "metadata": {},
   "source": [
    "## Sentence Probability and Perplexity Correlation\n",
    "\n",
    "\n",
    "In our study, we retrieve LLM probability and acceptability judgments of conditional statements through direct elicitation. Judgments could also be elicited through sentence probabilities (the probability a model assigns to a given string).\n",
    "\n",
    "Both direct elicitation and sentence probability are commonly used approaches to assess the probability that a model assigns to a given statement. While both methods come with advantages and disadvantages, there are key differences between the methods that should be considered:\n",
    "* **Sentence probability**:\n",
    "    * measures the probability a model assigns to a given string\n",
    "    * captures string form rather than semantics\n",
    "    * does not necessarily reflect how likely the model assesses the given statement to be true\n",
    "    * highly sensitive to paraphrasing and tokenization and exhibits a strong length bias\n",
    "* **Direct elicitation**:\n",
    "    * can be used to directly ask the model for the probability that a given statement is true\n",
    "    * especially for instruct/chat models, direct elicitation has been shown to be better calibrated than token-likelihoods, as it leverages the model’s internal latent knowledge and reasoning\n",
    "\n",
    "\n",
    "Nonetheless, we implement an additional analysis in this notebook, where we compare the sentence probability of both the conditional probability $P(B|A)$ and the if-probability $P($*If $A$, then $B$*$)$ to our directly elicited ratings. \\\n",
    "Because sentence probability is highly dependent on sentence length, we additionally compute the model’s perplexity over the statements. To study how well these probability-based judgements correlate with the model’s judgements obtained via direct elicitation, we compute both Pearson and Spearman correlations for each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da716473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(csv_filename, jsonl_filename, dir_varname, metric_varname):\n",
    "\n",
    "    dir_df = pd.read_csv(csv_filename, sep=\";\")\n",
    "    dir_df = dir_df.groupby('sample_number')[[dir_varname]].mean().reset_index()\n",
    "\n",
    "    collection = []\n",
    "    with open (jsonl_filename) as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            unnest = data [metric_varname]\n",
    "            collect = {\"prob\": unnest[\"prob\"],\n",
    "                    \"perplexity\": unnest[\"perplexity\"]\n",
    "            }\n",
    "            collection.append(collect)\n",
    "\n",
    "    new_df = pd.DataFrame(collection)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"dir_elicit\": dir_df[dir_varname].values,\n",
    "        \"model_prob\": new_df[\"prob\"].values,\n",
    "        \"perplexity\": new_df[\"perplexity\"].values\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"sent_prob_pearson\": df[\"dir_elicit\"].corr(df[\"model_prob\"], method=\"pearson\"),\n",
    "        \"sent_prob_spearman\": df[\"dir_elicit\"].corr(df[\"model_prob\"], method=\"spearman\"),\n",
    "        \"perplex_pearson\": df[\"dir_elicit\"].corr(df[\"perplexity\"], method=\"pearson\"),\n",
    "        \"perplex_spearman\": df[\"dir_elicit\"].corr(df[\"perplexity\"], method=\"spearman\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"Llama 8B cond. prob.\", \"dataframe_llama3_context_vanilla.csv\",\n",
    "     \"llama3_8B_sentence_probs.jsonl\", \"c_prob\", \"sentence_conditional_probability\"),\n",
    "\n",
    "    (\"Llama 8B if-prob.\", \"dataframe_llama3_context_vanilla.csv\",\n",
    "     \"llama3_8B_sentence_probs.jsonl\", \"if_prob\", \"sentence_if_probability\"),\n",
    "\n",
    "    (\"Llama 70B cond. prob.\", \"dataframe_llama70b_context_vanilla.csv\",\n",
    "     \"llama31_70B_sentence_probs.jsonl\", \"c_prob\", \"sentence_conditional_probability\"),\n",
    "\n",
    "    (\"Llama 70B if-prob.\", \"dataframe_llama70b_context_vanilla.csv\",\n",
    "     \"llama31_70B_sentence_probs.jsonl\", \"if_prob\", \"sentence_if_probability\"),\n",
    "\n",
    "    (\"Qwen 7B cond. prob.\", \"dataframe_qwen2_context_vanilla.csv\",\n",
    "     \"qwen25_7B_sentence_probs.jsonl\", \"c_prob\", \"sentence_conditional_probability\"),\n",
    "\n",
    "    (\"Qwen 7B if-prob.\", \"dataframe_qwen2_context_vanilla.csv\",\n",
    "     \"qwen25_7B_sentence_probs.jsonl\", \"if_prob\", \"sentence_if_probability\"),\n",
    "\n",
    "    (\"Qwen 72B cond. prob.\", \"dataframe_qwen72b_context_vanilla.csv\",\n",
    "     \"qwen25_72B_sentence_probs.jsonl\", \"c_prob\", \"sentence_conditional_probability\"),\n",
    "\n",
    "    (\"Qwen 72B if-prob.\", \"dataframe_qwen72b_context_vanilla.csv\",\n",
    "     \"qwen25_72B_sentence_probs.jsonl\", \"if_prob\", \"sentence_if_probability\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5e4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for label, csv_fn, jsonl_fn, dir_var, metric_var in configs:\n",
    "    corr = compute_correlations(\n",
    "        csv_fn, jsonl_fn, dir_var, metric_var\n",
    "    )\n",
    "    corr[\"Model\"] = label\n",
    "    rows.append(corr)\n",
    "\n",
    "df_table = pd.DataFrame(rows).set_index(\"Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Sentence Probability          Perplexity         \n",
      "                                   Pearson Spearman    Pearson Spearman\n",
      "Model                                                                  \n",
      "Llama 8B cond. prob.                  0.15     0.25      -0.28    -0.32\n",
      "Llama 8B if-prob.                     0.19     0.34      -0.32    -0.48\n",
      "Llama 70B cond. prob.                 0.16     0.26      -0.28    -0.38\n",
      "Llama 70B if-prob.                    0.11     0.31      -0.23    -0.31\n",
      "Qwen 7B cond. prob.                   0.08     0.22      -0.17    -0.27\n",
      "Qwen 7B if-prob.                      0.07     0.20      -0.08    -0.27\n",
      "Qwen 72B cond. prob.                  0.16     0.33      -0.21    -0.36\n",
      "Qwen 72B if-prob.                     0.14     0.19      -0.19    -0.31\n"
     ]
    }
   ],
   "source": [
    "df_table = df_table[[\n",
    "    \"sent_prob_pearson\",\n",
    "    \"sent_prob_spearman\",\n",
    "    \"perplex_pearson\",\n",
    "    \"perplex_spearman\"\n",
    "]]\n",
    "\n",
    "df_table.columns = pd.MultiIndex.from_product(\n",
    "    [[\"Sentence Probability\", \"Perplexity\"],\n",
    "     [\"Pearson\", \"Spearman\"]]\n",
    ")\n",
    "\n",
    "print(df_table.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

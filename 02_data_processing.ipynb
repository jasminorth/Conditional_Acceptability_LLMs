{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9499efb3",
   "metadata": {},
   "source": [
    "# Conditional Acceptability in Large Language Models and Humans – Data Processing\n",
    "\n",
    "This notebook prepares judgement data from humans and language models for analysis. The human data section is typically run once, while the model section can be re-run for new outputs. Both datasets are processed into a shared dataframe format to enable direct comparison in statistical models.\n",
    "\n",
    "<br>\n",
    "\n",
    "| **Aspect** | **Human Judgement Data** | **LLM Judgement Data** |\n",
    "|------------|---------------------------|-------------------------|\n",
    "| **Source** | Skovgaard-Olsen et al. (2016), [OSF](https://osf.io/7axdv/files/osfstorage) | Outputs from the prompting step (`results_{metric}_(no_)context_{style}.jsonl`) |\n",
    "| **Files Used** | • `dat_finaldw1.csv`: includes P(B\\|A) and **Prob(If A, then B)**  <br> • `dat_finaldw2.csv`: includes P(B\\|A) and **Acc(If A, then B)** | One `.jsonl` file per configuration (metric/context/style) |\n",
    "| **Processing** | Rearranged into unified format for easier handling | Regex applied to extract numeric probability or acceptability judgements |\n",
    "\n",
    "\n",
    "#### Unified format\n",
    "Both datasets are converted to the same structure. Each row contains:\n",
    "+ **Identifiers:** `sample_number`, `scenario_number`, `statement_type`, `relation_type`\n",
    "+ **Judgements:**\n",
    "    + `c_prob`: P(B|A)\n",
    "    + `if_prob`: Prob(If A, then B)\n",
    "    + `if_acc` Acc(If A, then B)\n",
    "+ **Instance:** `instance_id`: participant ID (humans) or prompt cycle (LLMs)\n",
    "\n",
    "> For LLMs, `instance_id` reflects the prompt repetition index (1–5), since models are tested individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8adf29",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing all necessary packages for loading data, extracting scores, and handling dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1af82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab7f51",
   "metadata": {},
   "source": [
    "## Load Scenario Metadata and Intialise Reference Dataframe\n",
    "\n",
    "This function loads metadata about the statements such as statement type and relation type from `statements.jsonl` and initializes the base dataframe.\n",
    "\n",
    "The resulting dataframe serves as a reference structure for building both the human and model datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bcc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_base_dataframe():\n",
    "    \n",
    "    with open(\"statements.jsonl\", \"r\", encoding=\"utf8\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    # Extract the relevant fields into a list of dicts\n",
    "    extracted = [\n",
    "        {\n",
    "            \"sample_number\": item[\"sample_number\"],\n",
    "            \"scenario_number\": item[\"scenario_number\"],\n",
    "            \"statement_type\": item[\"statement_type\"],\n",
    "            \"relation_type\": item[\"relation_type\"]\n",
    "        }\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb90ea",
   "metadata": {},
   "source": [
    "## Human Data *(run once)*\n",
    "\n",
    "+ **Reads:** `dat_finaldw1.csv`, `dat_finaldw2.csv`\n",
    "+ **Produces:** `human_df`, `human_data.csv`\n",
    "\n",
    "This section processes human judgement data from Skovgaard-Olsen et al. (2016) into a structured format for comparison with LLM responses.\n",
    "\n",
    "##### Purpose\n",
    "\n",
    "Participants in the original study rated conditional statements (\"If A, then B\") for either **probability** or **acceptability** across various scenarios. This section extracts and formats those ratings for analysis alongside model outputs.\n",
    "\n",
    "##### Raw Data Summary\n",
    "\n",
    "- The original dataset (in R format) contains additional material not used here\n",
    "- We extract only the relevant data and structure it to match the LLM format, enabling consistent analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1945456",
   "metadata": {},
   "source": [
    "### Load and Format Human Judgement Data\n",
    "\n",
    "This function loads probability and acceptability judgements from the original study’s CSV files. Each entry is matched to its corresponding scenario in the reference dataframe (initialised from `statements.jsonl`) based on scenario number, statement type, and relation type.\n",
    "\n",
    "The resulting dataframe is structured identically to the LLM data, ensuring compatibility for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_human_judgement_data():\n",
    "    reference_df = initialize_base_dataframe()\n",
    "    all_rows = []\n",
    "\n",
    "    # Source files and associated judgement type\n",
    "    judgement_sources = [\n",
    "        (\"dat_finaldw1.csv\", \"prob\"),  # Probability judgements\n",
    "        (\"dat_finaldw2.csv\", \"acc\")    # Acceptability judgements\n",
    "    ]\n",
    "\n",
    "    for csv_file, metric in judgement_sources:\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            scenario_number = row[\"le_nr\"]\n",
    "            statement_type = row[\"type\"][-2:]  # Last two characters\n",
    "            relation_type = row[\"rel_cond\"]\n",
    "            c_prob = row.get(\"CgivenA\", None)\n",
    "            instance_id = row.get(\"lfdn\", None)\n",
    "\n",
    "            # Extract the relevant judgement based on the source\n",
    "            judgement = row.get(\"P\", None) if metric == \"prob\" else row.get(\"ACC\", None)\n",
    "\n",
    "            # Match against reference rows\n",
    "            mask = (\n",
    "                (reference_df[\"scenario_number\"] == scenario_number) &\n",
    "                (reference_df[\"statement_type\"] == statement_type) &\n",
    "                (reference_df[\"relation_type\"].str[:2] == relation_type)\n",
    "            )\n",
    "\n",
    "            for _, matched_row in reference_df[mask].iterrows():\n",
    "                entry = matched_row.to_dict()\n",
    "                entry[\"c_prob\"] = c_prob\n",
    "                entry[\"if_prob\"] = judgement if metric == \"prob\" else np.nan\n",
    "                entry[\"if_acc\"] = judgement if metric == \"acc\" else np.nan\n",
    "                entry[\"instance_id\"] = instance_id\n",
    "                all_rows.append(entry)\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "#load_human_judgement_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96ae99",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Load and process the human judgement data, then save the dataframe to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dataframe_human.csv.\n"
     ]
    }
   ],
   "source": [
    "human_df = load_human_judgement_data()\n",
    "\n",
    "human_df.to_csv(\"dataframe_human.csv\", encoding='utf-8', index=False, sep = \";\")\n",
    "print (f\"Successfully saved dataframe_human.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075d3e5",
   "metadata": {},
   "source": [
    "## Part 2: Model Data *(re-runnable)*\n",
    "\n",
    "+ **Reads:** `results_{metric}_(no_)context_{style}.jsonl`\n",
    "+ **Produces:** `model_df`, `eval_{model_name}_(no_)context_{style}.csv`\n",
    "\n",
    "This section processes LLM judgement data from the prompting step into a structured format for comparison with human responses.\n",
    "\n",
    "##### Purpose\n",
    "\n",
    "The language model was prompted to provide a numeric rating from 0-100 in response to one of the following:\n",
    "\n",
    "+ A conditional probability query (*Assume A. How probable is B?*)\n",
    "\n",
    "+ A conditional statement judgement (*How probable/acceptable is “If A, then B”?*)\n",
    "\n",
    "These outputs are parsed and formatted for analysis.\n",
    "\n",
    "##### Raw Data Summary\n",
    "\n",
    "- Model responses are expected to include a numeric score (0–100), typically at the beginning of the response.\n",
    "- Each statement was given to the model a total of five times (we will use the prompt cycle index (1-5) as an ID to mirror the human data's participant ID)\n",
    "- Each `.jsonl` file represents a specific configuration (combination of metric, context, and prompt style) for one model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57cb08",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "**Changeable Parameters:**\n",
    "- `context`: Whether context is included in the prompt (True/False)\n",
    "- `style`: The prompt style used (\"vanilla\", \"fewshot\", \"cot\")\n",
    "- `model_name`: Name of the model being evaluated (used for filenames)\n",
    "\n",
    "**Fixed Settings:**\n",
    "- `metrics`: List of evaluation metrics (`\"c_prob\"`, `\"if_prob\"`, `\"if_acc\"`)\n",
    "- `pattern`: Regex pattern to extract numeric scores from the model output\n",
    "- `evaluation_file`: Output file for evaluation results, based on model configuration\n",
    "\n",
    "**Purpose:**\n",
    "These parameters allow for flexible evaluation across different configurations, metrics, and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248152cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changeable:\n",
    "context = True      # Options: True = context included, False = context excluded\n",
    "style = \"cot\"   # Options: \"vanilla\", \"fewshot\", \"cot\"\n",
    "\n",
    "tested_model = \"llama70b\"     # Name of the evaluated model (used for filenames) # Options: llama3, qwen2, llama70b, qwen72b\n",
    "\n",
    "\n",
    "# Fixed:\n",
    "# All metrics that we need to iterate over\n",
    "metrics = [\"c_prob\", \"if_prob\", \"if_acc\"]\n",
    "\n",
    "# Regex pattern to match score from the model output (number after \"answer\" or \"assistant\", optional colon, case-insensitive)\n",
    "pattern = r\"(?:answer:|assistant)\\s*[:\\-]?\\s*(\\d+)\"\n",
    "\n",
    "# File to store evaluation results\n",
    "output_file = f\"dataframe_{tested_model}_context_{style}.csv\" if context else f\"dataframe_{tested_model}_no_context_{style}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917066a",
   "metadata": {},
   "source": [
    "### Extract Scores from Model Outputs\n",
    "\n",
    "This function reads `.jsonl` files containing model responses for a specific evaluation metric. It extracts numeric scores (0–100) using regex. If a valid score isn't found or is out of bounds, a warning is printed.\n",
    "\n",
    "Each line in the file corresponds to a sample; each sample has 5 responses. The function returns a dataframe with `sample_number`, `instance_id`, `metric`, and `judgement`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1c226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(metric): \n",
    "    filename = (\n",
    "        f\"output_{tested_model}_{metric}_context_{style}.jsonl\"\n",
    "        if context\n",
    "        else f\"output_{tested_model}_{metric}_no_context_{style}.jsonl\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        lines = [json.loads(line) for line in file]\n",
    "        outputs = [entry[\"output\"] for entry in lines]\n",
    "\n",
    "    for sample_number, output_list in enumerate(outputs):\n",
    "        for instance_id, response in enumerate(output_list):\n",
    "            # Try to match numeric score using regex\n",
    "            match = re.findall(pattern, response, flags=re.IGNORECASE)\n",
    "\n",
    "            judgement = np.nan # default value\n",
    "\n",
    "            if match:\n",
    "\n",
    "                if len(match) in {1, 4}: # we expect either 1 matched number (vanilla) or 4 matched numbers (few-shot, CoT)\n",
    "                    try:\n",
    "                        score = int(match[-1])      # Use the last matched number\n",
    "                        if 0 <= score <= 100:\n",
    "                            judgement = score\n",
    "                        else:\n",
    "                            print(f\"Invalid score in sample {sample_number}, instance {instance_id}: {response[-100:]}\")\n",
    "                    except ValueError:\n",
    "                        print(f\"Could not convert to int in sample {sample_number}, instance {instance_id}: {response[-100:]}\")\n",
    "                else:\n",
    "                    print(f\"Unexpected number of matches ({len(match)}) in sample {sample_number}, instance {instance_id}: {response[-100:]}\")\n",
    "            else:\n",
    "                print(f\"No match in sample {sample_number}, instance {instance_id}: {response[-100:]}\")\n",
    "\n",
    "\n",
    "        #        try:\n",
    "         #           score = int(match[-1])      # Use the last matched number\n",
    "          #          judgement = score if 0 <= score <= 100 else np.nan\n",
    "           #         if np.isnan(judgement):\n",
    "            #            print(f\"Invalid score in sample {sample_number}, instance {instance_id}: {response[:100]}\")\n",
    "             #   except ValueError:\n",
    "              #      print(f\"Could not convert to int in sample {sample_number}, instance {instance_id}: {response[:100]}\")\n",
    "               #     judgement = np.nan\n",
    "         #   else:\n",
    "          #      print(f\"No match in sample {sample_number}, instance {instance_id}: {response[:100]}\")\n",
    "           #     judgement = np.nan\n",
    "\n",
    "            rows.append({\n",
    "                \"sample_number\": sample_number,\n",
    "                \"instance_id\": instance_id,\n",
    "                \"metric\": metric,\n",
    "                \"judgement\": judgement\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7753c3",
   "metadata": {},
   "source": [
    "### Combine Model Scores Across Metrics\n",
    "\n",
    "This function iterates over all metrics (`c_prob`, `if_prob`, `if_acc`) and uses `extract_scores()` to extract model judgements from `.jsonl` files.\n",
    "\n",
    "Each judgement is mapped back onto the reference structure (`sample_number`, `scenario_number`, etc.), and the scores are inserted into the appropriate field. The resulting dataframe has the same format as the human data, which simplifies later comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1b0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_number</th>\n",
       "      <th>scenario_number</th>\n",
       "      <th>statement_type</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>c_prob</th>\n",
       "      <th>if_prob</th>\n",
       "      <th>if_acc</th>\n",
       "      <th>instance_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HH</td>\n",
       "      <td>POS</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HH</td>\n",
       "      <td>POS</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HH</td>\n",
       "      <td>POS</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HH</td>\n",
       "      <td>POS</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HH</td>\n",
       "      <td>POS</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>LH</td>\n",
       "      <td>IRR</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>LH</td>\n",
       "      <td>IRR</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>LH</td>\n",
       "      <td>IRR</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>LH</td>\n",
       "      <td>IRR</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>LH</td>\n",
       "      <td>IRR</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_number  scenario_number statement_type relation_type  c_prob  \\\n",
       "0                1                1             HH           POS      99   \n",
       "0                1                1             HH           POS      98   \n",
       "0                1                1             HH           POS      98   \n",
       "0                1                1             HH           POS      99   \n",
       "0                1                1             HH           POS      98   \n",
       "..             ...              ...            ...           ...     ...   \n",
       "143            144               12             LH           IRR      50   \n",
       "143            144               12             LH           IRR      42   \n",
       "143            144               12             LH           IRR      50   \n",
       "143            144               12             LH           IRR      50   \n",
       "143            144               12             LH           IRR      50   \n",
       "\n",
       "     if_prob  if_acc  instance_id  \n",
       "0         99      98            0  \n",
       "0         98     100            1  \n",
       "0         98      99            2  \n",
       "0         98      99            3  \n",
       "0         99     100            4  \n",
       "..       ...     ...          ...  \n",
       "143        0       0            0  \n",
       "143        4       0            1  \n",
       "143        4       0            2  \n",
       "143        0       0            3  \n",
       "143        4       0            4  \n",
       "\n",
       "[720 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model_judgement_data():\n",
    "    reference_df = initialize_base_dataframe()\n",
    "    combined_rows = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        judgement_df = extract_scores(metric)\n",
    "\n",
    "        for _, row in judgement_df.iterrows():\n",
    "            sample_number = row[\"sample_number\"]\n",
    "            instance_id = row[\"instance_id\"]\n",
    "            judgement = row[\"judgement\"]\n",
    "\n",
    "            key = sample_number, instance_id\n",
    "\n",
    "            if key not in combined_rows:\n",
    "                # Start from base row copy\n",
    "                base_row = reference_df.iloc[sample_number].copy()\n",
    "                # Initialize all metrics to NaN\n",
    "                base_row[\"c_prob\"] = np.nan\n",
    "                base_row[\"if_prob\"] = np.nan\n",
    "                base_row[\"if_acc\"] = np.nan\n",
    "                base_row[\"instance_id\"] = instance_id\n",
    "                combined_rows[key] = base_row\n",
    "\n",
    "            # Assign judgement to correct metric column\n",
    "            if metric == \"c_prob\":\n",
    "                combined_rows[key][\"c_prob\"] = judgement\n",
    "            elif metric == \"if_prob\":\n",
    "                combined_rows[key][\"if_prob\"] = judgement\n",
    "            elif metric == \"if_acc\":\n",
    "                combined_rows[key][\"if_acc\"] = judgement\n",
    "\n",
    "    return pd.DataFrame(list(combined_rows.values()))\n",
    "\n",
    "\n",
    "load_model_judgement_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae576523",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Load and process the model judgement data, then save the dataframe to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a564b396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dataframe_llama70b_context_cot.csv.\n"
     ]
    }
   ],
   "source": [
    "#execution cell\n",
    "model_df = load_model_judgement_data()\n",
    "\n",
    "model_df.to_csv(output_file, encoding='utf-8', index=False, sep = \";\")\n",
    "print (f\"Successfully saved {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
